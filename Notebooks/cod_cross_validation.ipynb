{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cod_otoliths_cross_validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Og-KDZgpPnjB2N8IgbMoxsSinUhD43VR",
      "authorship_tag": "ABX9TyO3+KlQxoucUgnyG5XO51VH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IverMartinsen/MastersThesis/blob/main/Notebooks/cod_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv7ccU44VcXp"
      },
      "source": [
        "###This notebook shows the k*l-fold cross-validation procedure conducted on the cod otolith data.\n",
        "-------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTqlaQ9OVt8j"
      },
      "source": [
        "First we clone the repository to gain access to necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tgvaVhm9sRT"
      },
      "source": [
        "!git clone https://github.com/IverMartinsen/MastersThesis.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTkJavubV-XA"
      },
      "source": [
        "Import modules and images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E8m84wDmZNR"
      },
      "source": [
        "import sys\n",
        "sys.path.append(r'/content/MastersThesis/Python')      \n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from modules.imageloader import load_images\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.layers import RandomZoom, RandomRotation, RandomFlip\n",
        "\n",
        "# turn off interactive plotting to avoid plots popping up during trials\n",
        "plt.ioff()\n",
        "\n",
        "# path to image folder\n",
        "path = r'/content/drive/MyDrive/standard_convex'\n",
        "\n",
        "img_size = (128, 128)\n",
        "img_shape = img_size + (3,)\n",
        "num_splits = 5\n",
        "batch_size = 32\n",
        "initial_epochs = 100\n",
        "\n",
        "# import images\n",
        "sets = load_images(path, img_size, 5, seed=123, mode='RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KZJsaMCWIiE"
      },
      "source": [
        "Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gltgDvyp7gk"
      },
      "source": [
        "# path to folder where output images and results are stored\n",
        "destination = r'/content/drive/MyDrive/Forsøk/Forsøk 19.08.2021 (notaugmented)'\n",
        "\n",
        "# make a new folder where learning curves are stored\n",
        "folder_name = 'Learning curves'\n",
        "os.makedirs(destination + '/' + folder_name, exist_ok=True)\n",
        "\n",
        "# create DataFrame for storing individal test scores for all trials\n",
        "individual_results = pd.DataFrame()\n",
        "\n",
        "# create DataFrame for storing class-wise accuracies for all trials\n",
        "summary_results = pd.DataFrame()\n",
        "\n",
        "# define counter to keep track of trials\n",
        "trial_num = 0\n",
        "\n",
        "# to avoid error on 'del model' on first trial we need to define a model\n",
        "model = None\n",
        "\n",
        "# iterate over test sets\n",
        "for test_ds in sets:\n",
        "    # iterate over remaining possible validation sets\n",
        "    for valid_ds in (ds for ds in sets if ds != test_ds):\n",
        "        \n",
        "        trial_num += 1\n",
        "        \n",
        "        # use remaining sets for training\n",
        "        generators = [ds for ds in sets if ds not in (test_ds, valid_ds)]\n",
        "        \n",
        "        x_tr = np.concatenate([generator['images'] for generator in generators])\n",
        "        y_tr = np.concatenate([generator['labels'] for generator in generators])\n",
        "        \n",
        "        x_va = valid_ds['images']\n",
        "        y_va = valid_ds['labels']\n",
        "\n",
        "        # With `clear_session()` called at the beginning,\n",
        "        # Keras starts with a blank state at each iteration\n",
        "        tf.keras.backend.clear_session()\n",
        "        \n",
        "        del model\n",
        "\n",
        "        # Initialize base model using Xception pretrained on imagenet data.\n",
        "        # By using include_top=False we only include the feature extraction layers. \n",
        "        base_model = tf.keras.applications.Xception(\n",
        "            input_shape=img_shape,\n",
        "            include_top=False,\n",
        "            weights='imagenet'\n",
        "            )\n",
        "\n",
        "        # Fine-tune from this layer onwards\n",
        "        fine_tune_at = 150\n",
        "\n",
        "        # Freeze all the layers before the `fine_tune_at` layer\n",
        "        for layer in base_model.layers[:fine_tune_at]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Define full model. By calling 'training=False' we make the base\n",
        "        # model run in inference mode, e.g. batch normalization parameters\n",
        "        # are not updated, and dropout is not being used. \n",
        "        inputs = tf.keras.Input(shape=img_shape)\n",
        "        x = preprocess_input(inputs)\n",
        "        x = base_model(x, training=False)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        # set a high base learning rate for initial training\n",
        "        base_learning_rate = 1e-3\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "        # set early stopping\n",
        "        callbacks = [tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)]\n",
        "\n",
        "        # fit model\n",
        "        history = model.fit(\n",
        "            x_tr,\n",
        "            y_tr,\n",
        "            epochs=initial_epochs,\n",
        "            validation_data=(x_va, y_va),\n",
        "            callbacks=callbacks\n",
        "            )\n",
        "\n",
        "        # store training progress\n",
        "        acc = history.history['accuracy']\n",
        "        val_acc = history.history['val_accuracy']\n",
        "\n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        epochs_run = len(acc)\n",
        "\n",
        "        # Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "        # since we passed `training=False` when calling it. This means that\n",
        "        # the batchnorm layers will not update their batch statistics.\n",
        "        # This prevents the batchnorm layers from undoing all the training\n",
        "        # we've done so far.\n",
        "        base_model.trainable = True\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            metrics=['accuracy'],\n",
        "        )\n",
        "\n",
        "        history_fine = model.fit(\n",
        "            x_tr,\n",
        "            y_tr,\n",
        "            epochs=100,\n",
        "            validation_data=(x_va, y_va),\n",
        "            callbacks=callbacks)\n",
        "\n",
        "        acc += history_fine.history['accuracy']\n",
        "        val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "        loss += history_fine.history['loss']\n",
        "        val_loss += history_fine.history['val_loss']\n",
        "\n",
        "        # plot loss and accuracy and save figure\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(acc, label='Training Accuracy')\n",
        "        plt.plot(val_acc, label='Validation Accuracy')\n",
        "        plt.ylim([0.8, 1])\n",
        "        plt.plot([epochs_run-1,epochs_run-1],\n",
        "                plt.ylim(), label='Start Fine Tuning')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(loss, label='Training Loss')\n",
        "        plt.plot(val_loss, label='Validation Loss')\n",
        "        plt.ylim([0, 1.0])\n",
        "        plt.plot([epochs_run-1,epochs_run-1],\n",
        "                plt.ylim(), label='Start Fine Tuning')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('epoch')\n",
        "        \n",
        "        plt.savefig(\n",
        "            fname=destination + '/' + folder_name + '/trial' + \n",
        "            str(trial_num))\n",
        "\n",
        "        # evaluate model on test set and store results in DataFrames\n",
        "        predictions = model.predict(test_ds['images'])\n",
        "        labels = predictions.round()\n",
        "        \n",
        "        dataframe = pd.DataFrame(\n",
        "                predictions.flatten(),\n",
        "                index=test_ds['filenames'],\n",
        "                columns=[trial_num])\n",
        "        \n",
        "        individual_results = pd.merge(\n",
        "            individual_results, \n",
        "            dataframe, \n",
        "            how='outer', \n",
        "            left_index=True, \n",
        "            right_index=True\n",
        "            )\n",
        "        \n",
        "        # compute class-wise accuracy\n",
        "        idx = np.where(test_ds['labels'] == 0)\n",
        "\n",
        "        acc_0 = np.sum(\n",
        "            test_ds['labels'][idx] == labels.flatten()[idx]) / len(test_ds['labels'][idx])\n",
        "\n",
        "        idx = np.where(test_ds['labels'] == 1)\n",
        "\n",
        "        acc_1 = np.sum(\n",
        "            test_ds['labels'][idx] == labels.flatten()[idx]) / len(test_ds['labels'][idx])\n",
        "        \n",
        "        dataframe = pd.DataFrame(\n",
        "                [model.evaluate(test_ds['images'], test_ds['labels'])[1], acc_0, acc_1],\n",
        "                index=['Accuracy', 'ncc', 'neac'],\n",
        "                columns=[trial_num])\n",
        "        \n",
        "        summary_results = pd.merge(\n",
        "            summary_results, \n",
        "            dataframe, \n",
        "            how='outer', \n",
        "            left_index=True, \n",
        "            right_index=True)\n",
        "\n",
        "# save results to files\n",
        "individual_results.to_excel(destination + '/individual_results.xlsx')\n",
        "\n",
        "summary_results.to_excel(destination + '/summary_results.xlsx')\n",
        "\n",
        "individual_scores = (-np.log(1 / individual_results - 1))\n",
        "\n",
        "individual_scores.to_excel(destination + '/individual_scores.xlsx')\n",
        "\n",
        "individual_means = pd.DataFrame(\n",
        "    np.vstack((np.mean(individual_results, axis = 1), np.mean(individual_scores, axis = 1))).tranpose(), \n",
        "    index=individual_results.index, \n",
        "    columns=['Mean probability', 'Mean score']\n",
        "    )\n",
        "\n",
        "individual_means.to_excel(destination + '/individual_means.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}