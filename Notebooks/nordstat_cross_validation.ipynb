{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "klfold_trial_script.ipynb",
      "provenance": [],
      "mount_file_id": "1eIxYUwXj4muAigx92m0fnBBwy-IJYuHm",
      "authorship_tag": "ABX9TyMI3ybXAkMQACwSwX/0Ek+7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IverMartinsen/MastersThesis/blob/main/Notebooks/nordstat_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-KX7I8edh-q"
      },
      "source": [
        "###This notebook shows the k*l-fold cross-validation procedure applied on cod otolith images using a CNN.\n",
        "---------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_epsW3Jr5Tw"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Python scripts')\n",
        "\n",
        "import os      \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from imageloader import imageloader\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, ReLU\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import Input, SeparableConv2D, GlobalAveragePooling2D\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomZoom\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "def build_model():\n",
        "    L2 = tf.keras.regularizers.L2()\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 1))\n",
        "   \n",
        "    x = Rescaling(1./255)(inputs)\n",
        "    \n",
        "    x = Conv2D(32, 3, padding='same', kernel_regularizer=L2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Conv2D(32, 3, padding='same', kernel_regularizer=L2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Conv2D(32, 3, padding='same', kernel_regularizer=L2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    x = MaxPool2D()(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, kernel_regularizer=L2)(x)\n",
        "    x = ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    x = Dense(64, kernel_regularizer=L2)(x)\n",
        "    x = ReLU()(x)\n",
        "    \n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "        \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            learning_rate=1e-3),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "plt.ioff()\n",
        "\n",
        "'''\n",
        "Import images\n",
        "'''\n",
        "path = '/content/drive/MyDrive/Data/Torskeotolitter/standard'\n",
        "\n",
        "k = 5\n",
        "sets = imageloader(path, (128, 128), 5, seed=123)\n",
        "\n",
        "'''\n",
        "Train model\n",
        "'''\n",
        "#sets = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "destination = '/content/drive/MyDrive/Fors√∏k'\n",
        "\n",
        "folder_name = 'Learning curves'\n",
        "os.makedirs(destination + '/' + folder_name, exist_ok=True)\n",
        "\n",
        "\n",
        "individual_results = pd.DataFrame()\n",
        "summary_results = pd.DataFrame()\n",
        "trial_num =0\n",
        "model = None\n",
        "for test_ds in sets:\n",
        "    for valid_ds in (ds for ds in sets if ds != test_ds):\n",
        "        trial_num += 1\n",
        "        \n",
        "        generators = [ds for ds in sets if ds not in (test_ds, valid_ds)]\n",
        "        \n",
        "        x_tr = np.concatenate([generator['images'] for generator in generators])\n",
        "        y_tr = np.concatenate([generator['labels'] for generator in generators])\n",
        "        \n",
        "        x_va = valid_ds['images']\n",
        "        y_va = valid_ds['labels']\n",
        "\n",
        "        x_te = test_ds['images']\n",
        "        y_te = test_ds['labels']\n",
        "        f_te = test_ds['filenames']\n",
        "        \n",
        "        tf.keras.backend.clear_session()\n",
        "        \n",
        "        del model\n",
        "\n",
        "        model = build_model()\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                patience=20, restore_best_weights=True)]\n",
        "\n",
        "        history = model.fit(\n",
        "            x_tr,\n",
        "            y_tr,\n",
        "            epochs=1000,\n",
        "            validation_data=(x_va, y_va),\n",
        "            callbacks=callbacks)\n",
        "            \n",
        "\n",
        "        '''Plot loss and save figure'''\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(history.history['loss'], label='Training loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "        plt.plot(np.argmin(history.history['val_loss']), \n",
        "                 np.min(history.history['val_loss']),\n",
        "                 marker='o',\n",
        "                 label='Minimum validation loss')\n",
        "        plt.legend()\n",
        "        plt.xlabel('epochs')\n",
        "        plt.savefig(fname=destination + '\\\\' + folder_name + '\\\\trial' + str(trial_num))\n",
        "\n",
        "        '''\n",
        "        Evaluate on test set\n",
        "        '''\n",
        "        \n",
        "        #model.evaluate(test_ds)\n",
        "        \n",
        "        predictions = model.predict(x_te)\n",
        "        labels = predictions.round()\n",
        "        \n",
        "        \n",
        "        dataframe = pd.DataFrame(\n",
        "                y_te,\n",
        "                index=f_te,\n",
        "                columns=[trial_num])\n",
        "        \n",
        "        dataframe[trial_num] = dataframe[trial_num] == labels.flatten()\n",
        "        \n",
        "        dataframe = dataframe*1\n",
        "        \n",
        "        individual_results = pd.merge(\n",
        "            individual_results, dataframe, how='outer', left_index=True, right_index=True)\n",
        "        \n",
        "        idx = np.where(y_te == 0)\n",
        "\n",
        "        acc_0 = np.sum(y_te[idx] == labels.flatten()[idx]) / len(y_te[idx])\n",
        "\n",
        "        idx = np.where(y_te == 1)\n",
        "\n",
        "        acc_1 = np.sum(y_te[idx] == labels.flatten()[idx]) / len(y_te[idx])\n",
        "\n",
        "        \n",
        "        dataframe = pd.DataFrame(\n",
        "                [model.evaluate(x_te, y_te)[1], acc_0, acc_1],\n",
        "                index=['Accuracy', 'cc', 'neac'],\n",
        "                columns=[trial_num])\n",
        "        \n",
        "        summary_results = pd.merge(\n",
        "            summary_results, dataframe, how='outer', left_index=True, right_index=True)\n",
        "\n",
        "individual_results.to_excel(destination + '\\\\individual_results.xlsx')\n",
        "summary_results.to_excel(destination + '\\\\summary_results.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}